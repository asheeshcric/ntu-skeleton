{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTU RGB-D 120 data path\n",
    "\n",
    "data_dir = '/data/zak/graph/ntu/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The train directory contains video samples from 120 classes with around 114480 samples (some might be missing though)\n",
    "* Each file name is in the following format:\n",
    "    - `S013C003P037R001A004.skeleton.npy`\n",
    "    - S013 stands for **Setup Number 13**\n",
    "    - C003 stands for **Camera Number 03**\n",
    "    - P037 stands for **Participant Number 037**\n",
    "    - R001 stands for **Replication Number (001 or 002 only) << need to find out what this means**\n",
    "    - A004 stands for **Action Class Number 004 (brush hair in this case)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design data loader for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A list of parameters that can be changed for the dataloader with their default values\n",
    "- kp_shape = (25,3)\n",
    "- seg_size = varies based on the action being performed\n",
    "- participant_list <= those who are in the train or validation or test set (a list of numbers/codes for the participants)\n",
    "- data_path = '/data/zak/graph/ntu/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTUDataset:\n",
    "    \n",
    "    def __init__(self, data_path, sample_set, kp_shape=(25, 3), seg_size=61):\n",
    "        # Initialize all parameters for the model\n",
    "        self.sample_set = sample_set\n",
    "        self.kp_shape = kp_shape\n",
    "        self.seg_size = seg_size\n",
    "        self.data_path = data_path\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of samples in the dataset\n",
    "        return len(self.sample_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return a particular item from the dataset\n",
    "        sample_name = self.sample_set[idx]\n",
    "        sample_path = os.path.join(self.data_path, sample_name)\n",
    "        \n",
    "        # Process the sample into tensor keypoints for the given index\n",
    "        sample_kp, action_class = self.read_sample(sample_path, sample_name)\n",
    "        \n",
    "    def read_sample(self, sample_path, sample_name):\n",
    "        data = np.load(sample_path, allow_pickle=True).item()\n",
    "        # Each data sample has the following keys:\n",
    "        # dict_keys(['file_name', 'nbodys', 'njoints', 'skel_body0', 'rgb_body0', 'depth_body0', 'skel_body1', 'rgb_body1', 'depth_body1'])\n",
    "        \n",
    "        \n",
    "    \n",
    "    # ----- Helper functions -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions for Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# samples file_name = 'S018C001P042R002A120.skeleton.npy'\n",
    "# P042 is the participant number\n",
    "# Remember that I am trying to split the dataset based on the participants and not the total samples\n",
    "# This means that the validation set will have samples from all unique participants that are not involved in the train set\n",
    "\n",
    "def get_participant_number(file_name):\n",
    "    return sample.split('P')[1][:3]\n",
    "\n",
    "def split_participants(data_path, val_pct=0.2):\n",
    "    # Returns a random list of participants for the train and validation sets respectively\n",
    "    samples = os.listdir(data_path)\n",
    "    total_samples = len(samples)\n",
    "    # Get all unique participant numbers\n",
    "    all_participants = set()\n",
    "    for sample in samples:\n",
    "        part = get_participant_number(sample)\n",
    "        all_participants.add(part)\n",
    "    total_participants = len(all_participants)\n",
    "    all_participants = list(all_participants)\n",
    "    \n",
    "    # Split into train and val sets\n",
    "    val_len = int(total_participants * val_pct)\n",
    "    # Randomly shuffle the list\n",
    "    random.shuffle(list(all_participants))\n",
    "    train_participants = all_participants[val_len:]\n",
    "    val_participants = all_participants[:val_len]\n",
    "\n",
    "    print(f'Total Video Samples: {len(samples)} || Total Participants: {len(all_participants)} || Train Participants: {len(train_participants)} || Validation Participants: {len(val_participants)}')\n",
    "    return train_participants, val_participants\n",
    "\n",
    "def get_train_val_set(data_path, val_pct=0.2):\n",
    "    train_participants, val_participants = split_participants(data_path, val_pct)\n",
    "    train_samples, val_samples = [], []\n",
    "    for sample in os.lisdir(data_path):\n",
    "        participant_number = get_participant_number(sample)\n",
    "        if participant_number in val_participants:\n",
    "            val_samples.add(sample)\n",
    "        else:\n",
    "            train_samples.add(sample)\n",
    "            \n",
    "    return train_samples, val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu",
   "language": "python",
   "name": "ntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
